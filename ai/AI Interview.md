# AI Interview

## ReLU

ReLu能够改善但不能解决梯度消失的问题。

改善：改善了Sigmoid等函数的梯度过小，导致反向传播下降严重的问题。

未解决：

+ 梯度消失除了激励函数的梯度问题，还可能因为权重矩阵导致，这个没有办法解决
+ ReLU函数存在“死区”，因此本身会带来一定梯度的损耗（Leaky ReLU可以解决这个问题）

## CNN

卷积神经网络，顾名思义，就是以卷积操作为核心的神经网络。它具有参数少、泛化能力强、多层次特征表达等特性，在计算机视觉领域取得了巨大的成果，最近的一些成果表面，它在自然语言处理等领域也拥有不错的潜力。

卷积神经网络主要包括卷积、非线性激励函数、池化等技术。

### Technology

#### Convolution

卷积分为卷和积

+ 积：使用卷积核与相同大小的图像区域进行乘积，然后求和。
+ 卷：将上述操作在全图范围内执行

优点：

+ 通过卷积操作可以实现局部连接、权重共享，这大大减少了参数（当然，在一些特定的任务中，也可以使用Local Conv，即每个神经元都使用私有的卷积核，这能提高模型的表达能力，但对于输入图像有更为严苛的要求）
+ 且带来了一定的平移不变性。

缺点：

+ 边缘像素的权重过小。
+ 图像被动缩小，在深度网络中，导致边缘特征消失。

为了解决这个问题，当前比较通用的做法是采用padding，即在图像外围新增一定的填充像素，这可以确保原始图像的每个像素点参与卷积的次数相同，且图像不会被动的缩小。此处之所以说被动，是因为当卷积的步长大于1时，图像会被主动的缩小。

#### Non-Liner Activation

卷积操作本身是线性的，而根据矩阵运算可知，多层线性计算与单层线性计算是可以等价，因此深度就失去意义，使得模型的表达能力有限，因此需要引入非线性变换。

激励函数接在卷积操作之后，对卷积的每一个输出值都进行非线性变换。通常有sigmoid、relu等。目前最广泛使用的是relu，因为其不易过拟合，运算速度快，不存在梯度消失等优良特性。

#### Pooling

由于图像的像素很大，因此尽管采用了局部连接和共享权重，计算开销仍然很大。但图像任务存在一个共性，即相邻的特征是非常近似的，因此如果将相邻的特征进行一个合并，那么可以大大减少计算量，这就是池化。

池化主要有三个作用：

+ 维度缩减，减少计算量。
+ 增加模型的平移不变性

### Improvement

#### 尺寸自适应

卷积神经网络通常会后接一个全连接层，用于学习特征的相关性并输出最终的结果。由于全连接层需要固定的输入，因此导致卷积神经网络只能处理固定尺寸的图像。解决这个问题的办法通常是采用特殊的池化。

+ 空间金字塔池化、Roi Pooling、Global Pooling。

通过这些池化之后，任意尺寸的图像经过神经网络后，都可以输出固定维度的特征。

#### 特征融合

卷积神经网络的不同层具有不同抽象层次的特征，在网络的开始处特征往往是具象的，局部的，如一些边缘，纹理。而随着网络加深，特征变得抽象，且全局化，如猫，狗等。

原始的卷积神经网络，由于全连接层只与卷积层的末尾相连，因此只能利用到抽象的特征，没有充分利用浅层的一些细节。

这方面比较前沿的技术是FPN，图像金字塔网络。它将不同层次的特征进行一个融合，更好的利用了抽象与具象的信息，在物体识别等领域取得了很好的效果。

#### 广度

卷积神经网络在学习率等参数外，还有许多额外的超参数如卷积核的尺寸、步长、池化层的尺寸等。Google提出的Inception网络，即将多个不同结构的卷积网络横向叠在一起，同时训练多个网络，并自动调节对不同子网络的权重。

#### 深度

普通的卷积神经网络深度往往在几层到十几层。更深的网络就会导致过拟合，梯度消失等问题。

这方面主要的解决方案是采用跳跃连接，代表有ResNet和DenseNet，它们在不相邻的卷积层之间构建跳跃连接，这种跳跃连接使得反向传播的时候梯度能够进行跳跃传递，从而一定程度上减少了衰减。

另一方面，跳跃连接使得网络可以轻易的学习到一种恒等式，使得网络的层数看似很深，实则并不会带来过拟合。



### Application

卷积神经网络之所以得到大规模的应用很大程度上是由于其易于迁移学习的特性。这是因为它具有层级化的特征，在不同的CV任务中，底层特征往往是大同小异的，因此可以使用预训练的优秀模型然后进行精调即可得到不错的效果。

#### 风格识别

利用卷积神经网络的不同层次的特征来表示图像的不同尺度的风格。并使用两幅图像的风格的差异作为损失，来自动生成一副融合了二者风格的图像。

#### 图像分类

在卷积层后接全连接层即可。

#### 对象检测

Faster-RCNN：使用CNNs提取特征，然后产生两个分支，一个分支接全连接层预测对象的大致区域。然后再将制定区域的特征输出输出到另外一个分支的全连接层中进行更精细的分类和定位。

#### 对象分割

MASK-RCNN：在Faster-RCNN网络中添加一个并行的Mask分支，采用全卷积网络，和反卷积技术，来生成对应的Mask。



## RNN

![ä» RNN, LSTM, GRU å° SRU](https://pic4.zhimg.com/v2-174e0527b59b338671dc0640dfcc71b3_1200x500.jpg) 

RNN全称循环神经网络，它不同于传统神经网络只能从前一层获取输入，它还能记录本层在上一个时刻的状态，即在时间维度上进行建模。由于这个特性，RNN通常用于语音识别，自然语言处理等序列任务中。

RNN的缺陷：

+ RNN和其他神经网络一样面临梯度消失的问题，解决办法是ReLu或者LSTM等技术。
+ RNN只能利用过去的信息，解决的办法是BIRNN（双向RNN）
+ RNN难以记录遥远的信息，并难以主动忘记邻近的信息，解决办法神LSTM



## LSTM

![[å­¦ä¹ ç¬è®°] Long Short-Term Memory](https://pic2.zhimg.com/v2-4d3169af17390c5cbfec347f375b8387_1200x500.jpg) 



+ 遗忘门：例如遇到句号，忘记之前的句子。
+ 记忆门：选择记忆的程度，例如主语可能需要更关注。
+ 更新门：用于获取输出，判断cell中的信息有多少是本时刻需要的



