# 概率论

##贝叶斯公式

${\displaystyle P(A|B)={\frac {P(A)\times P(B|A)}{P(B)}}}$

贝叶斯定理是关于随机事件A和B的条件概率的一则定理。


其中P(A|B)是在B发生的情况下A发生的可能性。

在贝叶斯定理中，每个名词都有约定俗成的名称：

- P(*A*|*B*)是已知B发生后A的条件概率，也由于得自B的取值而被称作A的后验概率。
- P(*A*)是A的先验概率（或边缘概率）。之所以称为"先验"是因为它不考虑任何B方面的因素。
- P(*B*|*A*)是已知A发生后B的条件概率，也由于得自A的取值而被称作B的后验概率，又称似然函数。
- P(*B*)是B的先验概率或边缘概率。




## 极大似然估计（Maximum likelihood estimation）

简化概念：利用已知结果，反推最大概率导致这样结果的参数。

$\theta_{ML}(x) = arg max_\theta f(x|\theta)$



## MLE与机器学习

将贝叶斯公式写为如下形式，以更好地从机器学习的角度去理解：

$P(x,y | \theta) = \cfrac{P(x,y)\times P(\theta | x,y) }{P(\theta)}$

 $P(x, y | \theta)$ 表示，参数为 $\theta$ 时，输入为$x$ 输出为 $y$ 的概率。

机器学习的目的是找到参数为 $\theta$ 输入为 $x$ 时，概率最大的输出 $y$ ， 即使 $P(x, y | \theta)$  最大时对应的 $y$ 

由于$P(x, y)$ 和 $P(\theta)$ 是先验知识，在系统不变的情况下值不变，因此该问题等价于

$P(\theta|x,y)$ 最大，即当输入为$x$ 输出为 $y$ 时，概率最大的参数 $\theta$ 



## 极大后验估计（Maximum a posteriori estimation）

在极大似然估计的基础上，假设$\theta$ 存在一个先验分布$g$ 。

$\theta_{MAP}(x) = arg max_{\theta} f(x|\theta)g(\theta)$

相当于规则化（正则化，regularization）的MLE。

机器学习中的正则化也是引入先验知识。