# 信息论

##自信息

$$I(x) = -\log^{P(x)}_b$$

概越小，信息量越大



## 香农熵

$$H(x) = -\sum_iP(x_i)log_b^{P(x_i)} = E[I(x)]$$ 

压缩率下限，b为2时单位为比特。



##相对熵

又称KL散度（KL divergence)、信息散度、信息增益，用于表示两个分布之间的差异。

对于离散事件：

$$D_{KL}(P||Q) = \sum_iP(i)ln\cfrac{P(i)}{Q(i)} = E_P[\log_b\cfrac{P(x)}{Q(x)}] = E_P[I_Q(x) - I_P(x)] = H(P,Q) - H(P)$$

对于连续事件：

$$D_{KL}(P||Q) = \int p(x)\log(\cfrac{p(x)}{q(x)}) $$ 



1、当P = Q，$$D_KL(P||Q) = 0$$

2、$$D_{KL}(P||Q)  \neq D_{KL}(Q||P)$$



## 交叉熵

$$H(P, Q) = - \sum P(i)\log(Q(i))$$

$$H(P, Q) = D_{KL}(P||Q) + H(P)$$


## 熵、交叉熵、KL散度



+ Entropy $$H(x)$$：对分布P进行编码的最小位元数
+ KL divergence $$D_{KL}(P||Q)$$：使用Q的编码来对P进行编码的额外位元数
+ Cross-entropy $$H(P, Q)$$：使用Q的编码来对P进行编码的平均位元数