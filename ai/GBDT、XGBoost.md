# GBDT、XGBoost总结

## GBDT

基于回归树（拟合残差）。采用加法模型，训练多棵树然后加权平均。每一颗新树的目标值是之前N-1棵树在损失函数上泰勒展开后的一阶梯度。



## XGBoost

XGBoost主要是对GBDT进行了如下改进：

+ 支持线性分类器
+ 目标函数加上了正则项目：树的复杂度以及叶子节点值的L2正则。
+ 额外使用了泰勒展开之后二阶导数
+ 学习率：削弱每一颗树的影响，通常把eta设置小一点，迭代次数大一点。
+ 列抽样：借鉴随机森林，提高泛化能力，且加速计算
+ 支持缺失值处理：
  + GBDT中需要先根据规则填充缺失值
  + XGBoost中，只对Non-missing数据进行切割点划分，然后根据增益来确定缺失值的划分方向。在测试时缺失值自动分到右子树
+ 分裂点选择：
  + GBDT使用贪心算法进行遍历，在特征巨大的时候开销非常大
  + XGBoost将每个特征的所有值排序，然后根据百分位法选择有限的候选分割点（且特征排序后存在内存中，在迭代过程中，可以反复使用）
+ 并行化：XGBoost和GBDT都只能逐个树进行训练，但是XGBoost采用了并行计算划分和增益然后进行合并的方法。